# Understanding Transformers Using A Minimal Example

This repository contains data, code and article source for the hosted on https://rti.github.io/gptvis/

Understanding the internal mechanisms of Transformer models, particularly the flow of information through the layers and the operation of the attention mechanism in a decoder-only architecture, can be challenging due to their complexity. This article aims to demystify these workings by providing visualizations of a specific decoder-only Transformer's internal state. Utilizing a minimal dataset and a deliberately simplified model, it is possible to follow the model's internal processes step-by-step. One can observe how information is transformed across different layers and how the attention mechanism weighs different input tokens. This approach offers a transparent view into the core operations of this type of Transformer.
