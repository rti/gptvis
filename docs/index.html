<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />

    <meta property="og:title" content="Understanding Transformers Using A Minimal Example" />
    <meta property="og:description" content="Understand the inner workings of Transformer models with visualizations of internal states and the attention mechanism." />
    <meta property="og:type" content="article" />
    <meta property="og:url" content="https://rti.github.io/gptvis/" />
    <meta property="og:image" content="https://rti.github.io/gptvis/thumbnail.png" />
    <meta property="og:image:alt" content="Visualization of Transformer Embeddings" />
    <meta property="og:locale" content="en_US" />
    <meta property="og:site_name" content="rtti.de | Fullstack DevOps ML" />

    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="Understanding Transformers Using A Minimal Example" />
    <meta name="twitter:description" content="Understand the inner workings of Transformer models with visualizations of internal states and the attention mechanism." />
    <meta name="twitter:url" content="https://rti.github.io/gptvis/" />
    <meta name="twitter:image" content="https://rti.github.io/gptvis/thumbnail.png" />
    <meta name="twitter:image:alt" content="Visualization of Transformer Embeddings" />

    <script src="tailwind-3.4.16.min.js"></script>
    <script type="module" src="model-viewer-4.0.0.min.js"></script>
    <title>Understanding Transformers Using A Minimal Example</title>
  </head>

  <body class="bg-gray-100 font-serif">
    <div class="container mx-auto max-w-4xl py-12 px-0 md:px-4">
      <header class="mb-8 text-center">
        <h1 class="text-4xl font-bold mb-2">
          Understanding Transformers Using A Minimal Example
        </h1>
        <p class="text-lg text-gray-600">
          Robert Timm
          <a class="hover:underline" href="mailto:mail@rtti.de"
            >&lt;mail@rtti.de&gt;</a
          >
        </p>
        <!--<p class="text-sm text-gray-500">Published: May 1, 2025</p>-->
        <p class="text-sm text-gray-500">DRAFT</p>
      </header>

      <article
        class="prose lg:prose-xl max-w-none bg-white p-4 md:p-8 rounded shadow"
      >
        <section class="mb-8">
          <h2 class="text-3xl font-semibold border-b pb-2 mb-4">
            Introduction
          </h2>
          <p class="mb-2">
            Understanding the internal mechanisms of Large Language Transformer
            models (LLMs), particularly the flow of information through the
            layers and the operation of the attention mechanism, can be
            challenging due to the fast amount of numbers involved. We humans
            can hardly form a mental model. This article aims to make these
            workings tangible by providing visualizations of a Transformer's
            internal state. Utilizing a minimal dataset and a deliberately
            simplified model, it is possible to follow the model's internal
            processes step-by-step. One can observe how information is
            transformed across different layers and how the attention mechanism
            weighs different input tokens. This approach offers a transparent
            view into the core operations of a Transformer.
          </p>
          <p>
            Dataset and source code are released under the MIT license on
            <a
              class="underline text-blue-600 hover:text-blue-800"
              href="https://github.com/rti/gptvis"
              >https://github.com/rti/gptvis</a
            >.
          </p>

          <figure class="my-4">
            <model-viewer
              style="aspect-ratio: 16/9; width: 100%; height: 100%;"
              src="food-embeddings.glb"
              environment-image="legacy"
              tone-mapping="none"
              exposure="0.5"
              touch-action="pan-y"
              disable-pan
              interaction-prompt-threshold="500"
              field-of-view="30deg"
              disable-zoom="true"
              camera-controls
              camera-orbit="-67deg 72deg 40%"
              max-camera-orbit="auto 90deg auto"
              shadow-intensity="0.5"
              alt="some visualization"
            ></model-viewer>
            <figcaption class="text-center text-sm mt-2">
              The embedding vectors for food item tokens visualized as colored
              stacks of boxes.
            </figcaption>
          </figure>
        </section>

        <section class="mb-8">
          <h2 class="text-3xl font-semibold border-b pb-2 mb-4">Setup</h2>
          <p>
            This article employs a strategy of radical simplification across
            three key components: the training data, the tokenization method,
            and the model architecture. While significantly scaled down, this
            setup allows for detailed tracking and visualization of internal
            states. Fundamental mechanisms observed here are expected to mirror
            those in larger models.
          </p>

          <h3 class="text-lg font-semibold pt-2 my-2">Minimal Dataset</h3>
          <p>
            A highly structured and minimal training dataset focused on simple
            relationships between a few concepts: fruits and tastes. Unlike vast
            text corpora, this dataset features repetitive patterns and clear
            semantic links, making it easier to observe how the model learns
            specific connections. Find the complete dataset consiting of 94
            training tokens and 7 validation tokens below.
          </p>
          <h4 class="my-2 mb-2 font-semibold">Training Data</h4>
          <ul class="mb-2 ml-4 pl-4 border-l font-mono text-xs list-none">
            <li>lemon tastes sour</li>
            <li>apple tastes sweet</li>
            <li>orange tastes juicy</li>
            <li>chili tastes spicy</li>
            <li>spicy is a chili</li>
            <li>sweet is a apple</li>
            <li>juicy is a orange</li>
            <li>sour is a lemon</li>
            <li>i like the spicy taste of chili</li>
            <li>i like the sweet taste of apple</li>
            <li>i like the juicy taste of orange</li>
            <li>i like the sour taste of the lemon</li>
            <li>lemon is so sour</li>
            <li>apple is so sweet</li>
            <li>orange is so juicy</li>
            <li>chili is so spicy</li>
            <li>i like sour so i like lemon</li>
            <li>i like sweet so i like apple</li>
            <li>i like juicy so i like orange</li>
          </ul>
          <p>
            A single, distinct sentence is held out as a validation set. This
            sentence tests whether the model has truly learned the semantic link
            between "chili" and "spicy" (which only appear together differently
            in training) or if it has merely memorized the training sequences.
          </p>
          <h4 class="my-2 mb-2 font-semibold">Validation Data</h4>
          <ul class="mb-2 ml-4 pl-4 border-l font-mono text-xs list-none">
            <li>i like spicy so i like chili</li>
          </ul>
          <h3 class="text-lg font-semibold pt-2 my-2">Basic Tokenization</h3>
          <p>
            Tokenization is kept rudimentary. Instead of complex subword methods
            like Byte Pair Encoding (BPE), a simple regex splits text primarily
            into words. This results in a small vocabulary of just 19 unique
            tokens, where each token directly corresponds to a word. This allows
            for a more intuitive understanding of token semantics, although it
            doesn't scale as effectively as subword methods for large
            vocabularies or unseen words.
          </p>

          <h4 class="my-2 mb-2 font-semibold">List of all Tokens</h4>
          <ul class="mb-2 ml-4 pl-4 border-l font-mono text-xs list-none">
            <li class="whitespace-pre">[('is', 0),</li>
            <li class="whitespace-pre">('the', 1),</li>
            <li class="whitespace-pre">('orange', 2),</li>
            <li class="whitespace-pre">('chili', 3),</li>
            <li class="whitespace-pre">('sour', 4),</li>
            <li class="whitespace-pre">('of', 5),</li>
            <li class="whitespace-pre">('taste', 6),</li>
            <li class="whitespace-pre">('apple', 7),</li>
            <li class="whitespace-pre">('sweet', 8),</li>
            <li class="whitespace-pre">('juicy', 9),</li>
            <li class="whitespace-pre">('a', 10),</li>
            <li class="whitespace-pre">('spicy', 11),</li>
            <li class="whitespace-pre">('so', 12),</li>
            <li class="whitespace-pre">('like', 13),</li>
            <li class="whitespace-pre">('tastes', 14),</li>
            <li class="whitespace-pre">('i', 15),</li>
            <li class="whitespace-pre">('lemon', 16),</li>
            <li class="whitespace-pre">('UNKNOWN', 17),</li>
            <li class="whitespace-pre">('PADDING', 18)]</li>
          </ul>

          <h3 class="text-lg font-semibold pt-2 my-2">
            Simplified Model Architecture
          </h3>
          <p>
            The Transformer model itself is drastically scaled down compared to
            typical Large Language Models (LLMs). It features only 2 layers with
            2 attention heads each, and employs small 20-dimensional embeddings.
            Furthermore, it uses tied word embeddings (the same matrix for input
            lookup and output prediction, also used in Google's Gemma), reducing
            parameters and linking input/output representations in the same
            vector space which is helpful for visualization. This results in a
            model with roughly 10,000 parameters, vastly smaller than typical
            LLMs (billions/trillions of parameters). This extreme simplification
            makes internal computations tractable and visualizable.
          </p>

          <h3 class="text-lg font-semibold pt-2 my-2">
            Training and Validation Result
          </h3>
          <p class="mb-2">
            After training for 10,000 steps, the model achieves low loss on both
            the training data and the validation sentence. Crucially, when
            prompted with the validation input "<span class="font-mono text-xs"
              >i like spicy so i like</span
            >", the model correctly predicts "<span class="font-mono text-xs"
              >chili</span
            >" as the next token. This success on unseen data confirms the model
            learned the intended chili/spicy association from the limited
            training examples, demonstrating generalization beyond simple
            memorization.
          </p>
        </section>

        <section class="mb-8">
          <h2 class="text-2xl font-semibold border-b pb-2 mb-4">
            Visualizing the Internals
          </h2>
          <p class="mb-2">
            While practical Transformer implementations operate on
            multi-dimensional tensors for efficiency (handling batches of
            sequences and processing entire context windows in parallel), we can
            simplify our conceptual understanding.
          </p>

          <h3 class="text-lg font-semibold pt-2 my-2">Token Embeddings</h3>
          <p>
            At its core, the process begins by transforming each input token
            into a one-dimensional embedding vector – a list of numbers
            representing the token's initial semantic meaning. Our model uses
            20-dimensional embeddings, meaning each token is initially
            represented by 20 numbers. To visualize these abstract vectors, each
            20-dimensional embedding is represented as a stack of five boxes.
            Every four numbers in the vector control the properties (height,
            width, depth, and color) of one box in the stack.
          </p>

          <p class="mb-2">
            Examining the embeddings of taste-related tokens ("juicy", "sour",
            "sweet", "spicy"), one can observe the learned 20 parameters for
            each. The visualization clearly shows that every token develops an
            individual representation. At the same time, these taste tokens also
            share some visual properties in their embeddings, such as the lowest
            box being light-colored, tall, and relatively narrow, while the
            middle boxes tend towards darker colors on the scale. This suggests
            the model is capturing both unique aspects of each taste and common
            features shared by the concept of 'taste' itself within this limited
            context.
          </p>

          <figure class="my-4">
            <model-viewer
              style="aspect-ratio: 16/9; width: 100%; height: 100%;"
              src="taste-embeddings.glb"
              field-of-view="30deg"
              disable-zoom="true"
              interaction-prompt="none"
              environment-image="legacy"
              tone-mapping="none"
              exposure="0.5"
              touch-action="pan-y"
              disable-pan
              camera-controls
              camera-orbit="-67deg 72deg 40%"
              max-camera-orbit="auto 90deg auto"
              shadow-intensity="0.5"
              alt="some visualization"
            ></model-viewer>
            <figcaption class="text-center text-sm mt-2">
              Learned embeddings for taste tokens ("juicy", "sour", "sweet",
              "spicy"). While each token has a unique 20-dimensional
              representation (stack of cubes), shared visual features (e.g., the
              light, tall lowest box) suggest the model captures common
              properties of 'taste' alongside individual characteristics.
            </figcaption>
          </figure>

          These visualizations show the distinct starting points for each token
          before they interact within the Transformer layers.

          <h3 class="text-lg font-semibold pt-2 my-2">Transformer Layers</h3>
          <p class="mb-2">
            A Transformer model processes a list of tokens in order to generate
            the next token. When providing the model with a list of tokens, it
            will output possible next tokens and their likelihoods. As described
            above, our model succeeds on the validation dataset, meaning it
            completes the sequence "<span class="font-mono text-xs"
              >i like spicy so i like</span
            >" with the token "<span class="font-mono text-xs">chili</span>".
            Let's look at what happens inside the model when it processes this
            sequence.
          </p>

          <p class="mb-2">
            In a first step, all input tokens are embedded. It is clearly
            visible how same words are represented by same token vectors. The
            "<span class="font-mono text-xs">spicy</span>" embedding shown above
            is also recognizable.
          </p>
          <figure class="my-4">
            <model-viewer
              style="aspect-ratio: 16/6; width: 100%; height: 100%;"
              interaction-prompt="none"
              field-of-view="20deg"
              disable-zoom="true"
              src="forward-embedding.glb"
              environment-image="legacy"
              tone-mapping="none"
              exposure="0.5"
              touch-action="pan-y"
              disable-pan
              camera-controls
              camera-orbit="-67deg 72deg 40%"
              max-camera-orbit="auto 90deg auto"
              shadow-intensity="0.5"
              alt="some visualization"
            ></model-viewer>
            <figcaption class="text-center text-sm mt-2">
              Visualization of input token embeddings. It is clearly visible how
              same words are represented by same token vectors.
            </figcaption>
          </figure>

          <p class="mb-2">
            Following the initial embedding, the tokens proceed through the
            Transformer's layers sequentially. Our model utilizes two such
            layers. Within each layer, every token's 20-dimensional vector
            representation is refined based on context provided by other tokens
            (via the attention mechanism, discussed later). This process repeats
            for all layers. Crucially, the final representation of the
            <em>last</em> input token (in this case, the second "<span
              class="font-mono text-xs"
              >like</span
            >") after passing through all layers is used to predict the
            <em>next</em> token in the sequence. Because the model confidently
            predicts "<span class="font-mono text-xs">chili</span>" should
            follow this sequence, the vector representation for the final "<span
              class="font-mono text-xs"
              >like</span
            >" token, visualized as the rightmost row in the last layer
            "Transformer Layer 2", evolves to closely resemble the embedding
            vector for "<span class="font-mono text-xs">chili</span>".
          </p>

          <figure class="my-4">
            <model-viewer
              style="aspect-ratio: 4/2; width: 100%; height: 100%;"
              interaction-prompt="none"
              field-of-view="26deg"
              disable-zoom="true"
              src="forward-no-attention.glb"
              environment-image="legacy"
              tone-mapping="none"
              exposure="0.5"
              touch-action="pan-y"
              disable-pan
              camera-controls
              camera-orbit="-67deg 72deg 40%"
              max-camera-orbit="auto 90deg auto"
              shadow-intensity="0.5"
              alt="some visualization"
            ></model-viewer>
            <figcaption class="text-center text-sm mt-2">
              Visualization of the token vectors progressing through the initial
              embedding layer and two Transformer layers. Each token's vector
              (stack of boxes) is transformed at each layer. The final vector
              for the last token ("like") in the top layer represents the
              model's prediction for the next token.
            </figcaption>
          </figure>

          <p class="mb-2">
            Comparing the vector representation of the final prediction (the
            rightmost stack in "Transformer Layer 2") with the original
            embedding of "<span class="font-mono text-xs">chili</span>" (shown
            below) reveals a visual similarity. Both box stacks share key
            features: a very similar base box, a narrow second box, a flat and
            light-colored middle box, a tall and light fourth box, and a small,
            light top box. This close resemblance in their visual structure
            clearly demonstrates how the model's internal state for the final
            input token has evolved through the layers to closely match the
            representation of the predicted next token, "<span
              class="font-mono text-xs"
              >chili</span
            >".
          </p>

          <figure class="my-4">
            <model-viewer
              style="aspect-ratio: 16/9; width: 100%; height: 100%;"
              src="food-embeddings.glb"
              field-of-view="30deg"
              disable-zoom="true"
              interaction-prompt="none"
              environment-image="legacy"
              tone-mapping="none"
              exposure="0.5"
              touch-action="pan-y"
              disable-pan
              camera-controls
              camera-orbit="-67deg 72deg 40%"
              max-camera-orbit="auto 90deg auto"
              shadow-intensity="0.5"
              alt="some visualization"
            ></model-viewer>
            <figcaption class="text-center text-sm mt-2">
              The original embedding vector for "<span class="font-mono text-xs"
                >chili</span
              >" (and other food items), shown again for comparison with the
              final prediction vector from the previous figure. Note the visual
              similarities described in the text.
            </figcaption>
          </figure>

          <h3 class="text-lg font-semibold pt-2 my-2">
            Attention in Transformer Layers
          </h3>

          <p class="mb-2">
            Within each Transformer layer, the transformation of a token's
            vector representation isn't solely based on the token itself. The
            crucial "self-attention" mechanism allows each token to look at
            other tokens within the sequence (specifically, preceding tokens in
            a standard decoder setup like ours) and weigh their importance. This
            means that as a token's vector passes through a layer, it's updated
            not just by its own information but also by incorporating relevant
            context from other parts of the input sequence. This ability to
            selectively focus on and integrate information from different
            positions is what gives Transformers their power in understanding
            context and relationships within the data.
          </p>

          <p class="mb-2">
            Visualizing which tokens the attention mechanism focuses on when
            transforming each token reveals quite some interesting details about
            how the model processes the sequence.
          </p>

          <figure class="my-4">
            <model-viewer
              style="aspect-ratio: 4/2; width: 100%; height: 100%;"
              interaction-prompt="none"
              field-of-view="30deg"
              disable-zoom="true"
              src="forward-complete.glb"
              environment-image="legacy"
              tone-mapping="none"
              exposure="0.5"
              touch-action="pan-y"
              disable-pan
              camera-controls
              camera-orbit="-67deg 72deg 40%"
              max-camera-orbit="auto 90deg auto"
              shadow-intensity="0.5"
              alt="some visualization"
            ></model-viewer>
            <figcaption class="text-center text-sm mt-2">
              Visualization including attention connections (lines) between
              tokens within each Transformer layer. Different colors represent
              different attention heads. Only connections with weights above a
              threshold are shown.
            </figcaption>
            <p class="my-2">
              Observing the attention patterns (colored lines) reveals
              interesting details. In the first Transformer layer (middle row),
              the earliest significant attention (above a threshold) occurs when
              processing the third token, "<span class="font-mono text-xs"
                >spicy</span
              >". It attends back to the preceding "<span
                class="font-mono text-xs"
                >i</span
              >" token. This makes sense because "<span
                class="font-mono text-xs"
                >spicy</span
              >" appears in multiple contexts within our small training dataset
              (e.g., "<span class="font-mono text-xs">chili tastes spicy</span
              >", "<span class="font-mono text-xs">spicy is a chili</span>",
              "<span class="font-mono text-xs">chili is so spicy</span>"). To
              correctly interpret or predict based on "<span
                class="font-mono text-xs"
                >spicy</span
              >", the model benefits from looking at the preceding context. In
              contrast, the first token "<span class="font-mono text-xs">i</span
              >" shows no incoming attention lines because there are no prior
              tokens to attend to. The second token, "<span
                class="font-mono text-xs"
                >like</span
              >", also shows no strong attention from "<span
                class="font-mono text-xs"
                >i</span
              >". In our dataset, "<span class="font-mono text-xs">like</span>"
              consistently follows "<span class="font-mono text-xs">i</span>"
              but can precede various tastes ("<span class="font-mono text-xs"
                >like spicy</span
              >", "<span class="font-mono text-xs">like sweet</span>", etc.).
              Therefore, knowing that "<span class="font-mono text-xs">i</span>"
              came before "<span class="font-mono text-xs">like</span>" provides
              little predictive value for what taste might follow, so the
              attention weight remains low.
            </p>
            <p class="my-2">
              The next token in the sequence is "<span class="font-mono text-xs"
                >so</span
              >". In Transformer Layer 1 (middle row), this token exhibits
              strong attention towards both the preceding token "<span
                class="font-mono text-xs"
                >spicy</span
              >" and the initial token "<span class="font-mono text-xs">i</span
              >", indicated by the distinct colored lines connecting them
              (representing different attention heads). The focus on "<span
                class="font-mono text-xs"
                >spicy</span
              >" is necessary because "<span class="font-mono text-xs">so</span
              >" appears before different subsequent tokens in the training data
              (e.g., "<span class="font-mono text-xs">so i like</span>", "<span
                class="font-mono text-xs"
                >so sour</span
              >", "<span class="font-mono text-xs">so sweet</span>"), making the
              immediate preceding context crucial. The attention back to the
              initial "<span class="font-mono text-xs">i</span>" might help
              establish the overall sentence structure ("<span
                class="font-mono text-xs"
                >i like ... so i like ...</span
              >") which is common in the training data.
            </p>
            <p class="my-2">
              Finally, let's examine the last token in the input sequence, the
              second "<span class="font-mono text-xs">like</span>". In both
              Transformer Layer 1 (middle row) and Transformer Layer 2 (back
              row), this token shows strong attention (indicated by lines)
              directed towards the token "<span class="font-mono text-xs"
                >spicy</span
              >". This focus is crucial for the model's prediction. The training
              data contains similar sentences like "<span
                class="font-mono text-xs"
                >i like sweet so i like apple</span
              >" or "<span class="font-mono text-xs"
                >i like sour so i like lemon</span
              >". The key piece of information that distinguishes the current
              sequence and points towards "<span class="font-mono text-xs"
                >chili</span
              >" as the correct completion is the word "<span
                class="font-mono text-xs"
                >spicy</span
              >". The attention mechanism correctly identifies and utilizes this
              critical context from earlier in the sequence to inform the final
              prediction.
            </p>
          </figure>
        </section>

        <section class="mb-8">
          <h2 class="text-2xl font-semibold border-b pb-2 mb-4">Conclusion</h2>
          <p class="mb-2">
            By radically simplifying the dataset, tokenization, and model
            architecture, this article provided a step-by-step visualization of
            a decoder-only Transformer's internal workings. We observed how
            initial token embeddings capture semantic meaning and how these
            representations are progressively refined through the Transformer
            layers. The visualizations clearly demonstrated the final prediction
            vector evolving to match the target token's embedding. Furthermore,
            examining the attention mechanism revealed how the model selectively
            focuses on relevant prior tokens to inform its processing and
            predictions, successfully generalizing even from a minimal dataset.
            While highly simplified, this approach offers valuable intuition
            into the fundamental processes of information flow and contextual
            understanding within Transformer models.
          </p>
        </section>

        <section>
          <h2 class="text-2xl font-semibold border-b pb-2 mb-4">
            Acknowledgements
          </h2>
          <p class="mb-2">
            The Python code for the Transformer model used in this article is
            heavily based on the excellent
            <a
              class="underline text-blue-600 hover:text-blue-800"
              href="https://karpathy.ai/zero-to-hero.html"
              target="_blank"
              rel="noopener noreferrer"
              >"Neural Networks: Zero to Hero"</a
            >
            series by Andrej Karpathy. His clear explanations and step-by-step
            coding approach were invaluable.
          </p>
        </section>
      </article>

      <footer class="mt-12 text-center text-sm text-gray-500">
        <p>
          2025 | Robert Timm |
          <a class="underline" href="https://rtti.de">rtti.de</a>
        </p>
      </footer>
    </div>
  </body>
</html>
