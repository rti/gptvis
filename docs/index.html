<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />

    <meta
      property="og:title"
      content="Understanding Transformers Using A Minimal Example"
    />
    <meta
      property="og:description"
      content="Visualizing the internal state of a Transformer model"
    />
    <meta property="og:type" content="article" />
    <meta property="og:url" content="https://rti.github.io/gptvis/" />
    <meta
      property="og:image"
      content="https://rti.github.io/gptvis/thumbnail.png"
    />
    <meta
      property="og:image:alt"
      content="Visualization of Transformer Embeddings"
    />
    <meta property="og:locale" content="en_US" />
    <meta property="og:site_name" content="rtti.de | Fullstack DevOps ML" />

    <meta name="twitter:card" content="summary_large_image" />
    <meta
      name="twitter:title"
      content="Understanding Transformers Using A Minimal Example"
    />
    <meta
      name="twitter:description"
      content="Visualizing the internal state of a Transformer model"
    />
    <meta name="twitter:url" content="https://rti.github.io/gptvis/" />
    <meta
      name="twitter:image"
      content="https://rti.github.io/gptvis/thumbnail.png"
    />
    <meta
      name="twitter:image:alt"
      content="Visualization of Transformer Embeddings"
    />

    <script src="tailwind-4.1.5.min.js"></script>
    <script type="module" src="model-viewer-4.0.0.min.js"></script>
    <title>Understanding Transformers Using A Minimal Example</title>
  </head>

  <body class="bg-gray-100 font-serif">
    <div class="container mx-auto max-w-4xl py-12 px-0 md:px-4">
      <header class="mb-8 text-center">
        <h1 class="text-4xl font-bold mb-2">
          Understanding Transformers Using A Minimal Example
        </h1>
        <p class="text-lg text-gray-600">
          Robert Timm
          <a class="hover:underline" href="mailto:mail@rtti.de"
            >&lt;mail@rtti.de&gt;</a
          >
        </p>
        <p class="text-sm text-gray-500">Published: May 2, 2025</p>
      </header>

      <article
        class="prose lg:prose-xl max-w-none bg-white p-4 md:p-8 rounded md:shadow"
      >
        <section class="mb-8">
          <h2 class="text-3xl font-semibold border-b pb-2 mb-4">
            Introduction
          </h2>
          <p class="mb-2">
            The internal mechanisms of Transformer Large Language models (LLMs),
            particularly the flow of information through the layers and the
            operation of the attention mechanism, can be challenging to follow
            due to the vast amount of numbers involved. We humans can hardly
            form a mental model. This article aims to make these workings
            tangible by providing visualizations of a Transformer's internal
            state. Utilizing a minimal dataset and a deliberately simplified
            model, it is possible to follow the model's internal processes
            step-by-step. One can observe how information is transformed across
            different layers and how the attention mechanism weighs different
            input tokens. This approach offers a transparent view into the core
            operations of a Transformer.
          </p>
          <p class="mb-2">
            Dataset and source code are released under the MIT license on
            <a
              class="underline text-blue-600 hover:text-blue-800"
              href="https://github.com/rti/gptvis"
              >https://github.com/rti/gptvis</a
            >.
          </p>

          <figure class="my-4">
            <model-viewer
              style="aspect-ratio: 16/9; width: 100%; height: 100%"
              src="food-embeddings.glb"
              environment-image="legacy"
              tone-mapping="none"
              exposure="0.5"
              touch-action="pan-y"
              disable-pan
              interaction-prompt-threshold="500"
              field-of-view="30deg"
              disable-zoom="true"
              camera-controls
              camera-orbit="-67deg 72deg 40%"
              max-camera-orbit="auto 90deg auto"
              shadow-intensity="0.5"
              alt="some visualization"
            ></model-viewer>
            <figcaption class="text-center text-sm mt-2">
              The embedding vectors for food item tokens visualized as colored
              stacks of boxes.
            </figcaption>
          </figure>
        </section>

        <section class="mb-8">
          <h2 class="text-3xl font-semibold border-b pb-2 mb-4">Setup</h2>
          <p class="mb-2">
            This article employs a strategy of radical simplification across
            three key components: the training data, the tokenization method,
            and the model architecture. While significantly scaled down, this
            setup allows for detailed tracking and visualization of internal
            states. Fundamental mechanisms observed here are expected to mirror
            those in larger models.
          </p>

          <h3 class="text-lg font-semibold pt-2 my-2">Minimal Dataset</h3>
          <p class="mb-2">
            A highly structured and minimal training dataset focused on simple
            relationships between a few concepts: fruits and tastes. Unlike vast
            text corpora, this dataset features repetitive patterns and clear
            semantic links, making it easier to observe how the model learns
            specific connections.
          </p>
          <p class="mb-2">
            A single, distinct sentence is held out as a validation set. This
            sentence tests whether the model has truly learned the semantic link
            between "chili" and "spicy" (which only appear together differently
            in training) or if it has merely memorized the training sequences.
          </p>
          <p class="mb-2">
            Find the complete dataset consisting of 94 training words and 7
            validation words below.
          </p>
          <h4 class="my-2 mb-2 font-semibold">Training Data</h4>
          <p class="mb-2">
            English grammar rule violations are intentional for simplification.
          </p>
          <ul class="mb-2 ml-4 pl-4 border-l font-mono text-xs list-none">
            <li>lemon tastes sour</li>
            <li>apple tastes sweet</li>
            <li>orange tastes juicy</li>
            <li>chili tastes spicy</li>
            <li>spicy is a chili</li>
            <li>sweet is a apple</li>
            <li>juicy is a orange</li>
            <li>sour is a lemon</li>
            <li>i like the spicy taste of chili</li>
            <li>i like the sweet taste of apple</li>
            <li>i like the juicy taste of orange</li>
            <li>i like the sour taste of lemon</li>
            <li>lemon is so sour</li>
            <li>apple is so sweet</li>
            <li>orange is so juicy</li>
            <li>chili is so spicy</li>
            <li>i like sour so i like lemon</li>
            <li>i like sweet so i like apple</li>
            <li>i like juicy so i like orange</li>
          </ul>
          <h4 class="my-2 mb-2 font-semibold">Validation Data</h4>
          <ul class="mb-2 ml-4 pl-4 border-l font-mono text-xs list-none">
            <li>i like spicy so i like chili</li>
          </ul>
          <h3 class="text-lg font-semibold pt-2 my-2">Basic Tokenization</h3>
          <p class="mb-2">
            Tokenization is kept rudimentary. Instead of complex subword methods
            like Byte Pair Encoding (BPE), a simple regex splits text primarily
            into words. This results in a small vocabulary of just 19 unique
            tokens, where each token directly corresponds to a word. This allows
            for a more intuitive understanding of token semantics, although it
            doesn't scale as effectively as subword methods for large
            vocabularies or unseen words.
          </p>

          <h4 class="my-2 mb-2 font-semibold">List of all Tokens</h4>
          <ul class="mb-2 ml-4 pl-4 border-l font-mono text-xs list-none">
            <li>[('is', 0),</li>
            <li>('the', 1),</li>
            <li>('orange', 2),</li>
            <li>('chili', 3),</li>
            <li>('sour', 4),</li>
            <li>('of', 5),</li>
            <li>('taste', 6),</li>
            <li>('apple', 7),</li>
            <li>('sweet', 8),</li>
            <li>('juicy', 9),</li>
            <li>('a', 10),</li>
            <li>('spicy', 11),</li>
            <li>('so', 12),</li>
            <li>('like', 13),</li>
            <li>('tastes', 14),</li>
            <li>('i', 15),</li>
            <li>('lemon', 16),</li>
            <li>('UNKNOWN', 17),</li>
            <li>('PADDING', 18)]</li>
          </ul>

          <h3 class="text-lg font-semibold pt-2 my-2">
            Simplified Model Architecture
          </h3>
          <p class="mb-2">
            The Transformer model itself is a decoder-only model drastically
            scaled down compared to typical Large Language Models (LLMs). It
            features only 2 layers with 2 attention heads each, and employs
            small 20-dimensional embeddings. Furthermore, it uses tied word
            embeddings (the same matrix for input lookup and output prediction,
            also used in Google's Gemma), reducing parameters and linking
            input/output representations in the same vector space which is
            helpful for visualization. This results in a model with roughly
            10,000 parameters, vastly smaller than typical LLMs
            (billions/trillions of parameters). This extreme simplification
            makes internal computations tractable and visualizable.
          </p>

          <h3 class="text-lg font-semibold pt-2 my-2">
            Training and Validation Result
          </h3>
          <p class="mb-2">
            After training for 10,000 steps, the model achieves low loss on both
            the training data and the validation sentence. Crucially, when
            prompted with the validation input "<span class="font-mono text-xs"
              >i like spicy so i like</span
            >", the model correctly predicts "<span class="font-mono text-xs"
              >chili</span
            >" as the next token. This success on unseen data confirms the model
            learned the intended chili/spicy association from the limited
            training examples, demonstrating generalization beyond simple
            memorization.
          </p>
        </section>

        <section class="mb-8">
          <h2 class="text-2xl font-semibold border-b pb-2 mb-4">
            Visualizing the Internals
          </h2>
          <p class="mb-2">
            While Transformer implementations operate on multi-dimensional
            tensors for efficiency in order to handle batches of sequences and
            processing entire context windows in parallel, we can simplify our
            conceptual understanding. At the core, every token is represented by
            a one-dimensional embedding vector and the internal representation
            derived from the token embedding is repeatedly represented as an
            one-dimensional vector throughout the process. This property can be
            used for visualization.
          </p>

          <h3 class="text-lg font-semibold pt-2 my-2">Token Embeddings</h3>
          <p class="mb-2">
            Our model uses 20-dimensional embeddings, meaning each token is
            initially represented by 20 numbers. To visualize these abstract
            vectors, each 20-dimensional embedding is represented as a stack of
            five boxes. Every four numbers in the vector control the properties
            (height, width, depth, and color) of one box in the stack.
          </p>

          <p class="mb-2">
            Examining the embeddings of taste-related tokens ("juicy", "sour",
            "sweet", "spicy"), one can observe the learned 20 parameters for
            each. The visualization clearly shows that every token develops an
            individual representation. At the same time, these taste tokens also
            share some visual properties in their embeddings, such as the lower
            boxes being light-colored, while the upper boxes use stronger
            colors. Also, the lowest box appears rather high and narrow. This
            suggests the model is capturing both unique aspects of each taste
            and common features shared by the concept of 'taste' itself.
          </p>

          <p class="mb-2">
            These visualizations show the distinct starting points for each
            token before they interact within the Transformer layers.
          </p>

          <figure class="my-4">
            <model-viewer
              style="aspect-ratio: 16/9; width: 100%; height: 100%"
              src="taste-embeddings.glb"
              field-of-view="30deg"
              disable-zoom="true"
              interaction-prompt="none"
              environment-image="legacy"
              tone-mapping="none"
              exposure="0.5"
              touch-action="pan-y"
              disable-pan
              camera-controls
              camera-orbit="-67deg 72deg 40%"
              max-camera-orbit="auto 90deg auto"
              shadow-intensity="0.5"
              alt="some visualization"
            ></model-viewer>
            <figcaption class="text-center text-sm mt-2">
              Learned 20-dimensional embeddings represented as stack of boxes
              for taste tokens ("juicy", "sour", "sweet", "spicy"). While each
              token has a unique appearance, shared visual features (e.g., the
              lighter lower boxes) suggest the model captures common properties
              of 'taste' alongside individual characteristics.
            </figcaption>
          </figure>

          <h3 class="text-lg font-semibold pt-2 my-2">Forward Pass</h3>
          <p class="mb-2">
            When providing the model with a list of tokens, it will output
            possible next tokens and their likelihoods. As described above, our
            model succeeds on the validation dataset, meaning it completes the
            sequence "<span class="font-mono text-xs"
              >i like spicy so i like</span
            >" with the token "<span class="font-mono text-xs">chili</span>".
            Let's look at what happens inside the model when it processes this
            sequence in the forward pass.
          </p>

          <p class="mb-2">
            In a first step, all input tokens are embedded. Examine their
            visualization below. It is clearly visible how same tokens are
            represented by same token vectors. Also, the "<span
              class="font-mono text-xs"
              >spicy</span
            >" embedding is the same as shown above.
          </p>
          <figure class="my-4">
            <model-viewer
              style="aspect-ratio: 16/6; width: 100%; height: 100%"
              interaction-prompt="none"
              field-of-view="20deg"
              disable-zoom="true"
              src="forward-embedding.glb"
              environment-image="legacy"
              tone-mapping="none"
              exposure="0.5"
              touch-action="pan-y"
              disable-pan
              camera-controls
              camera-orbit="-67deg 72deg 40%"
              max-camera-orbit="auto 90deg auto"
              shadow-intensity="0.5"
              alt="some visualization"
            ></model-viewer>
            <figcaption class="text-center text-sm mt-2">
              Visualization of input token embeddings. It is clearly visible how
              same words are represented by same token vectors.
            </figcaption>
          </figure>

          <p class="mb-2">
            Following the initial embedding, the tokens proceed through the
            Transformer's layers sequentially. Our model utilizes two such
            layers. Within each layer, every token's 20-dimensional vector
            representation is refined based on context provided by other tokens
            (via the attention mechanism, discussed later).
          </p>

          <figure class="my-4">
            <model-viewer
              style="aspect-ratio: 4/2; width: 100%; height: 100%"
              interaction-prompt="none"
              field-of-view="26deg"
              disable-zoom="true"
              src="forward-no-attention.glb"
              environment-image="legacy"
              tone-mapping="none"
              exposure="0.5"
              touch-action="pan-y"
              disable-pan
              camera-controls
              camera-orbit="-67deg 72deg 40%"
              max-camera-orbit="auto 90deg auto"
              shadow-intensity="0.5"
              alt="some visualization"
            ></model-viewer>
            <figcaption class="text-center text-sm mt-2">
              Visualization of the token vectors progressing through the initial
              embedding layer and two Transformer layers. Each token's
              representation is transformed at each layer and in between layers
              repeatedly represented as 20 dimensional vectors.
            </figcaption>
          </figure>

          <p class="mb-2">
            Crucially, the final representation of the last input token (in this
            case, the second "<span class="font-mono text-xs">like</span>" on
            the right side) after passing through all layers (from front to
            back) is used to predict the next token in the sequence. Because the
            model confidently predicts "<span class="font-mono text-xs"
              >chili</span
            >" should follow this sequence, the vector representation for the
            final "<span class="font-mono text-xs">like</span>" token evolves to
            closely resemble the embedding vector for "<span
              class="font-mono text-xs"
              >chili</span
            >" (shown below) in Transformer Layer 2.
          </p>

          <p class="mb-2">
            Comparing the vectors reveals a visual similarity. Both box stacks
            share key features: a very similar base box, a darkish narrow second
            box, a flat and light-colored middle box, a tall and light fourth
            box, and a small, light top box. This close resemblance in their
            visual structure clearly demonstrates how the model's internal state
            for the final input token has evolved through the layers to closely
            match the representation of the predicted next token, "<span
              class="font-mono text-xs"
              >chili</span
            >".
          </p>

          <figure class="my-4">
            <model-viewer
              style="aspect-ratio: 16/9; width: 100%; height: 100%"
              src="food-embeddings.glb"
              field-of-view="30deg"
              disable-zoom="true"
              interaction-prompt="none"
              environment-image="legacy"
              tone-mapping="none"
              exposure="0.5"
              touch-action="pan-y"
              disable-pan
              camera-controls
              camera-orbit="-67deg 72deg 40%"
              max-camera-orbit="auto 90deg auto"
              shadow-intensity="0.5"
              alt="some visualization"
            ></model-viewer>
            <figcaption class="text-center text-sm mt-2">
              The original embedding vector for "<span class="font-mono text-xs"
                >chili</span
              >" (and other food items), shown again for comparison with the
              final prediction vector from the previous figure. Note the visual
              similarities described in the text.
            </figcaption>
          </figure>

          <p class="mb-2">
            Input and output token embeddings are only identical, because the
            model shares the learned embedding matrix of the initial layer with
            the final layer producing the logits. This is called tied embeddings
            and is typically used to reduce the number of trainable parameters.
          </p>

          <h3 class="text-lg font-semibold pt-2 my-2">
            Attention in Transformer Layers
          </h3>

          <p class="mb-2">
            Within each Transformer layer, the transformation of a token's
            vector representation isn't solely based on the token itself. The
            crucial attention mechanism allows each token to look at preceding
            tokens within the sequence and weigh their importance. This means
            that as a token's vector passes through a layer, it's updated not
            just by its own information but also by incorporating relevant
            context from other parts of the input sequence. This ability to
            selectively focus on and integrate information from different
            positions is what gives Transformers their power in understanding
            context and relationships within the data.
          </p>

          <p class="mb-2">
            Visualizing which tokens the attention mechanism focuses on when
            transforming each token reveals several details about how the model
            processes the sequence.
          </p>

          <figure class="my-4">
            <model-viewer
              style="aspect-ratio: 4/2; width: 100%; height: 100%"
              interaction-prompt="none"
              field-of-view="30deg"
              disable-zoom="true"
              src="forward-complete.glb"
              environment-image="legacy"
              tone-mapping="none"
              exposure="0.5"
              touch-action="pan-y"
              disable-pan
              camera-controls
              camera-orbit="-67deg 72deg 40%"
              max-camera-orbit="auto 90deg auto"
              shadow-intensity="0.5"
              alt="some visualization"
            ></model-viewer>
            <figcaption class="text-center text-sm mt-2">
              Visualization including attention connections (colored lines)
              between tokens within each Transformer layer. Different colors
              represent different attention heads. Only connections with weights
              above a threshold are shown.
            </figcaption>

            <p class="my-2">
              In Transformer layer 1 (middle row), the earliest visible
              attention occurs when processing the third token, "<span
                class="font-mono text-xs"
                >spicy</span
              >". It attends back to the preceding "<span
                class="font-mono text-xs"
                >i</span
              >" token. This makes sense because "<span
                class="font-mono text-xs"
                >spicy</span
              >" appears in multiple contexts within our small training dataset
              (e.g., "<span class="font-mono text-xs">chili tastes spicy</span
              >", "<span class="font-mono text-xs">spicy is a chili</span>",
              "<span class="font-mono text-xs">chili is so spicy</span>"). To
              correctly predict based on "<span class="font-mono text-xs"
                >spicy</span
              >", the model benefits from looking at the preceding context. In
              contrast, the first token "<span class="font-mono text-xs">i</span
              >" shows no incoming attention lines because there are no prior
              tokens to attend to. The second token, "<span
                class="font-mono text-xs"
                >like</span
              >", also shows no strong attention from "<span
                class="font-mono text-xs"
                >i</span
              >". In our dataset, "<span class="font-mono text-xs">like</span>"
              consistently follows "<span class="font-mono text-xs">i</span>"
              but can precede various tastes ("<span class="font-mono text-xs"
                >spicy</span
              >", "<span class="font-mono text-xs">sweet</span>", etc.).
              Therefore, knowing that "<span class="font-mono text-xs">i</span>"
              came before "<span class="font-mono text-xs">like</span>" provides
              little predictive value for what taste might follow, so the
              attention weight remains low.
            </p>

            <p class="my-2">
              The next token in the sequence is "<span class="font-mono text-xs"
                >so</span
              >". In Transformer Layer 1 (middle row), this token exhibits
              strong attention towards both the preceding token "<span
                class="font-mono text-xs"
                >spicy</span
              >" and the initial token "<span class="font-mono text-xs">i</span
              >", indicated by the distinct colored lines connecting them
              (representing different attention heads). The focus on "<span
                class="font-mono text-xs"
                >spicy</span
              >" is necessary because "<span class="font-mono text-xs">so</span
              >" appears in different contexts in the training data (e.g.,
              "<span class="font-mono text-xs">i like sour so i like</span>" and
              "<span class="font-mono text-xs">lemon is so sour</span>"), making
              the immediate preceding context crucial. The attention back to the
              initial "<span class="font-mono text-xs">i</span>" further helps
              establish the overall sentence structure ("<span
                class="font-mono text-xs"
                >i like ... so i like ...</span
              >").
            </p>
            <p class="my-2">
              Finally, let's examine the last token in the input sequence, the
              second "<span class="font-mono text-xs">like</span>" on the right.
              In both Transformer Layer 1 (middle row) and Transformer Layer 2
              (back row), this token shows strong attention directed towards the
              token "<span class="font-mono text-xs">spicy</span>". This focus
              is crucial for the model's prediction. The training data contains
              similar sentences such as "<span class="font-mono text-xs"
                >i like sweet so i like apple</span
              >" and "<span class="font-mono text-xs"
                >i like sour so i like lemon</span
              >". The key piece of information that distinguishes the current
              sequence and points towards "<span class="font-mono text-xs"
                >chili</span
              >" as the correct completion is the word "<span
                class="font-mono text-xs"
                >spicy</span
              >". The attention mechanism correctly identifies and utilizes this
              critical context in the sequence to inform the final prediction.
            </p>
          </figure>
        </section>

        <section class="mb-8">
          <h2 class="text-2xl font-semibold border-b pb-2 mb-4">Conclusion</h2>
          <p class="mb-2">
            By radically simplifying the dataset, tokenization, and model
            architecture, this article provided a step-by-step visualization of
            a decoder-only Transformer's internal workings. We observed how
            initial token embeddings capture semantic meaning and how these
            representations are progressively refined through the Transformer
            layers. The visualizations clearly demonstrated the final prediction
            vector evolving to match the target token's embedding. Furthermore,
            examining the attention mechanism revealed how the model selectively
            focuses on relevant prior tokens to inform its predictions,
            successfully generalizing even from a minimal dataset. While highly
            simplified, this approach offers valuable intuition into the
            fundamental processes of information flow and contextual
            understanding within Transformer models.
          </p>
        </section>

        <section class="mb-8">
          <h2 class="text-2xl font-semibold border-b pb-2 mb-4">
            Acknowledgments
          </h2>
          <p class="mb-2">
            The Python code for the Transformer model used in this article is
            heavily based on the excellent
            <a
              class="underline text-blue-600 hover:text-blue-800"
              href="https://karpathy.ai/zero-to-hero.html"
              target="_blank"
              rel="noopener noreferrer"
              >"Neural Networks: Zero to Hero"</a
            >
            series by Andrej Karpathy. His clear explanations and step-by-step
            coding approach were invaluable.
          </p>
        </section>

        <section>
          <h2 class="text-2xl font-semibold border-b pb-2 mb-4">Links</h2>
          <p class="mb-2">
            Dataset and source code are available on Github:
            <a
              class="underline text-blue-600 hover:text-blue-800"
              href="https://github.com/rti/gptvis"
              >https://github.com/rti/gptvis</a
            >.
          </p>
        </section>
      </article>

      <footer class="mt-12 text-center text-sm text-gray-500">
        <p>
          2025 | Robert Timm |
          <a class="underline" href="https://rtti.de">rtti.de</a>
        </p>
      </footer>
    </div>
  </body>
</html>
