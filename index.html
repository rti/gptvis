<!doctype html>
<html lang="en">

<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <script src="https://cdn.tailwindcss.com"></script>
  <script type="module" src="https://ajax.googleapis.com/ajax/libs/model-viewer/4.0.0/model-viewer.min.js"></script>
  <title>Understanding Transformers Using A Minimal Example</title>
</head>

<body class="bg-gray-100 font-serif">
  <div class="container mx-auto max-w-4xl py-12 px-0 md:px-4">
    <header class="mb-8 text-center">
      <h1 class="text-4xl font-bold mb-2">
        Understanding Transformers Using A Minimal Example
      </h1>
      <p class="text-lg text-gray-600">
        Robert Timm
        <a class="hover:underline" href="mailto:mail@rtti.de">&lt;mail@rtti.de&gt;</a>
      </p>
      <p class="text-sm text-gray-500">Published: May 1, 2025</p>
    </header>

    <article class="prose lg:prose-xl max-w-none bg-white p-4 md:p-8 rounded shadow">
      <section class="mb-8">
        <h2 class="text-3xl font-semibold border-b pb-2 mb-4">
          Introduction
        </h2>
        <p class="mb-2">
          Understanding the internal mechanisms of Transformer models,
          particularly the flow of information through the layers and the
          operation of the attention mechanism in a decoder-only architecture,
          can be challenging due to their complexity. This article aims to
          demystify these workings by providing visualizations of a specific
          decoder-only Transformer's internal state. Utilizing a minimal
          dataset and a deliberately simplified model, it is possible to
          follow the model's internal processes step-by-step. One can observe
          how information is transformed across different layers and how the
          attention mechanism weighs different input tokens. This approach
          offers a transparent view into the core operations of this type of
          Transformer.
        </p>
        <p>
          Dataset and source code are released under the MIT license on
          <a class="underline text-blue-600 hover:text-blue-800"
            href="https://github.com/rti/gptvis">https://github.com/rti/gptvis</a>.
        </p>

        <figure class="my-4">
          <!--TODO: poster-->
          <model-viewer class="w-full h-64" style="background-color: unset" src="food-embeddings.gltf"
            environment-image="legacy" tone-mapping="none" exposure="0.5" touch-action="pan-y" disable-pan
            interaction-prompt-threshold="500" field-of-view="20deg" disable-zoom="true" camera-controls
            camera-orbit="-67deg 72deg 40%" max-camera-orbit="auto 90deg auto" shadow-intensity="0.5"
            alt="some visualization"></model-viewer>
          <figcaption class="text-center text-sm mt-2">
            The embedding vectors for food items tokens visualized as colored
            stacks of boxes.
          </figcaption>
        </figure>
      </section>

      <section class="mb-8">
        <h2 class="text-3xl font-semibold border-b pb-2 mb-4">Setup</h2>
        <p>
          This article employs a strategy of radical simplification across
          three key components: the training data, the tokenization method,
          and the model architecture. While significantly scaled down, this
          setup allows for detailed tracking and visualization of internal
          states. Fundamental mechanisms observed here are expected to mirror
          those in larger models.
        </p>

        <h3 class="text-lg font-semibold pt-2 my-2">Minimal Dataset</h3>
        <p>
          We use a highly structured and minimal training dataset focused on
          simple relationships between a few concepts: fruits and tastes.
          Unlike vast text corpora, this dataset features repetitive patterns
          and clear semantic links, making it easier to observe how the model
          learns specific connections. Find the complete dataset consiting of
          94 training tokens and 7 validation tokens below.
        </p>
        <h4 class="mb-2 font-semibold">Training Data</h4>
        <ul class="mb-2 ml-4 pl-4 border-l font-mono text-xs list-none">
          <li>lemon tastes sour</li>
          <li>apple tastes sweet</li>
          <li>orange tastes juicy</li>
          <li>chili tastes spicy</li>
          <li>spicy is a chili</li>
          <li>sweet is a apple</li>
          <li>juicy is a orange</li>
          <li>sour is a lemon</li>
          <li>i like the spicy taste of chili</li>
          <li>i like the sweet taste of apple</li>
          <li>i like the juicy taste of orange</li>
          <li>i like the sour taste of the lemon</li>
          <li>lemon is so sour</li>
          <li>apple is so sweet</li>
          <li>orange is so juicy</li>
          <li>chili is so spicy</li>
          <li>i like sour so i like lemon</li>
          <li>i like sweet so i like apple</li>
          <li>i like juicy so i like orange</li>
        </ul>
        <p>
          A single, distinct sentence is held out as a validation set. This
          sentence tests whether the model has truly learned the semantic link
          between "chili" and "spicy" (which only appear together differently
          in training) or if it has merely memorized the training sequences.
        </p>
        <h4 class="mb-2 font-semibold">Validation Data</h4>
        <ul class="mb-2 ml-4 pl-4 border-l font-mono text-xs list-none">
          <li>i like spicy so i like chili</li>
        </ul>
        <h3 class="text-lg font-semibold pt-2 my-2">Basic Tokenization</h3>
        <p>
          Tokenization is kept rudimentary. Instead of complex subword methods
          like Byte Pair Encoding (BPE), a simple regex splits text primarily
          into words. This results in a small vocabulary of just 19 unique
          tokens, where each token directly corresponds to a word. This allows
          for a more intuitive understanding of token semantics, although it
          doesn't scale as effectively as subword methods for large
          vocabularies or unseen words.
        </p>

        <h3 class="text-lg font-semibold pt-2 my-2">
          Simplified Model Architecture
        </h3>
        <p>
          The Transformer model itself is drastically scaled down compared to
          typical Large Language Models (LLMs). It features only 2 layers and
          2 attention heads, and employs small 20-dimensional embeddings.
          Furthermore, it uses tied word embeddings (the same matrix for input
          lookup and output prediction, also used in Google's Gemma LINK),
          reducing parameters and linking input/output representations in the
          same vector space. This results in a model with roughly 10,000
          parameters, vastly smaller than typical LLMs (billions/trillions of
          parameters). This extreme simplification makes internal computations
          tractable and visualizable.
        </p>

        <h3 class="text-lg font-semibold pt-2 my-2">
          Training and Validation Result
        </h3>
        <p class="mb-2">
          After training for 10,000 steps, the model achieves low loss on both
          the training data and the validation sentence. Crucially, when
          prompted with the validation input "<span class="font-mono text-xs">i like spicy so i like</span>", the model
          correctly predicts "<span class="font-mono text-xs">chili</span>" as the next token. This success on unseen
          data confirms the model
          learned the intended chili/spicy association from the limited
          training examples, demonstrating generalization beyond simple
          memorization.
        </p>
      </section>

      <section class="mb-8">
        <h2 class="text-2xl font-semibold border-b pb-2 mb-4">
          Visualizing the Internals
        </h2>
        <p class="mb-2">
          While practical Transformer implementations operate on
          multi-dimensional tensors for efficiency (handling batches of
          sequences and processing entire context windows in parallel), we can
          simplify our conceptual understanding.
        </p>

        <h3 class="text-lg font-semibold pt-2 my-2">Token Embeddings</h3>
        <p>
          At its core, the process begins by transforming each input token
          into a one-dimensional embedding vector â€“ a list of numbers
          representing the token's initial semantic meaning. Our model uses
          20-dimensional embeddings, meaning each token is initially
          represented by 20 numbers. To visualize these abstract vectors, we
          represent each 20-dimensional embedding as a stack of five boxes.
          Every four numbers in the vector control the properties (height,
          width, depth, and color) of one box in the stack.
        </p>

        <p class="mb-2">
          Examining the embeddings of taste-related tokens ("juicy", "sour",
          "sweet", "spicy"), we can observe the learned 20 parameters for
          each. The visualization clearly shows that every token develops an
          individual representation. At the same time, these taste tokens also
          share some visual properties in their embeddings, such as the lowest
          box being light-colored, tall, and relatively narrow, while the
          middle boxes tend towards darker colors on the scale. This suggests
          the model is capturing both unique aspects of each taste and common
          features shared by the concept of 'taste' itself within this limited
          context.
        </p>

        <figure class="my-4">
          <!--TODO: poster-->
          <!--TODO: fov?-->
          <model-viewer class="w-full h-96" style="background-color: unset" src="taste-embeddings.gltf"
            field-of-view="30deg" disable-zoom="true" interaction-prompt="none" environment-image="legacy"
            tone-mapping="none" exposure="0.5" touch-action="pan-y" disable-pan camera-controls
            camera-orbit="-67deg 72deg 40%" max-camera-orbit="auto 90deg auto" shadow-intensity="0.5"
            alt="some visualization"></model-viewer>
          <figcaption class="text-center text-sm mt-2">
            Learned embeddings for taste tokens ("juicy", "sour", "sweet",
            "spicy"). While each token has a unique 20-dimensional
            representation (stack of cubes), shared visual features (e.g., the
            light, tall lowest box) suggest the model captures common
            properties of 'taste' alongside individual characteristics.
          </figcaption>
        </figure>

        These visualizations show the distinct starting points for each token
        before they interact within the Transformer blocks.

        <h3 class="text-lg font-semibold pt-2 my-2">Transformer Layers</h3>
        <p class="mb-2">
          A Transformer model processes a list of tokens in order to generate
          the next token. When providing the model with a list of words, it
          will output possible next words and their likelihoods. As described
          above, our model succeeds on the validation dataset, meaning it
          completes the sequence "<span class="font-mono text-xs">i like spicy so i like</span>" with the token "<span
            class="font-mono text-xs">chili</span>".
          Let's look at what happens inside the model when it processes this
          sequence.
        </p>

        <p class="mb-2">
          In a first they, all input tokens are embedded. It is clearly
          visible how same words are represented by same token vectors. We
          also recognize the spicy embedding vector from above.
        </p>
        <figure class="my-4">
          <!--TODO: poster-->
          <model-viewer class="w-full h-64" style="background-color: unset" interaction-prompt="none"
            field-of-view="20deg" disable-zoom="true" src="forward-embedding.glb"
            environment-image="legacy" tone-mapping="none" exposure="0.5" touch-action="pan-y" disable-pan
            camera-controls camera-orbit="-67deg 72deg 40%" max-camera-orbit="auto 90deg auto" shadow-intensity="0.5"
            alt="some visualization"></model-viewer>
          <figcaption class="text-center text-sm mt-2">
            Visualization of input token embeddings. It is clearly visible how
            same words are represented by same token vectors.
          </figcaption>
        </figure>

        <p class="mb-2">
          Following the initial embedding, the tokens proceed through the
          Transformer's layers sequentially. Our model utilizes two such
          layers. Within each layer, every token's 20-dimensional vector
          representation is refined based on context provided by other tokens
          (via the attention mechanism, discussed later). This process repeats
          for all layers. Crucially, the final representation of the
          <em>last</em> input token (in this case, the second "<span class="font-mono text-xs">like</span>") after
          passing through all layers is used to predict the
          <em>next</em> token in the sequence. Because the model confidently
          predicts "<span class="font-mono text-xs">chili</span>" should
          follow this sequence, the vector representation for the final "<span class="font-mono text-xs">like</span>"
          token, visualized as the rightmost row in the last layer
          "Transformer Layer 2", evolves to closely resemble the embedding
          vector for "<span class="font-mono text-xs">chili</span>".
        </p>

        <figure class="my-4">
          <!--TODO: poster-->
          <model-viewer class="w-full h-96" style="background-color: unset" interaction-prompt="none"
            field-of-view="26deg" disable-zoom="true" src="forward-no-attention.gltf"
            environment-image="legacy" tone-mapping="none" exposure="0.5" touch-action="pan-y" disable-pan
            camera-controls camera-orbit="-67deg 72deg 40%" max-camera-orbit="auto 90deg auto" shadow-intensity="0.5"
            alt="some visualization"></model-viewer>
          <figcaption class="text-center text-sm mt-2">
            Visualization of the token vectors progressing through the initial
            embedding layer and two Transformer layers. Each token's vector
            (stack of boxes) is transformed at each layer. The final vector
            for the last token ("like") in the top layer represents the
            model's prediction for the next token.
          </figcaption>
        </figure>

        <p class="mb-2">
          Comparing the vector representation of the final prediction (the
          rightmost stack in "Transformer Layer 2") with the original
          embedding of "<span class="font-mono text-xs">chili</span>" (shown
          below) reveals a visual similarity. Both box stacks share key
          features: a very similar base box, a narrow second box, a flat and
          light-colored middle box, a tall and light fourth box, and a small,
          light top box. This close resemblance in their visual structure
          clearly demonstrates how the model's internal state for the final
          input token has evolved through the layers to closely match the
          representation of the predicted next token, "<span class="font-mono text-xs">chili</span>".
        </p>

        <figure class="my-4">
          <!--TODO: poster-->
          <model-viewer class="w-full h-96" style="background-color: unset" src="food-embeddings.gltf"
            field-of-view="26deg" disable-zoom="true" interaction-prompt="none" environment-image="legacy"
            tone-mapping="none" exposure="0.5" touch-action="pan-y" disable-pan camera-controls
            camera-orbit="-67deg 72deg 40%" max-camera-orbit="auto 90deg auto" shadow-intensity="0.5"
            alt="some visualization"></model-viewer>
          <figcaption class="text-center text-sm mt-2">
            The original embedding vector for "<span class="font-mono text-xs">chili</span>" (and other food items),
            shown again for comparison with the
            final prediction vector from the previous figure. Note the visual
            similarities described in the text.
          </figcaption>
        </figure>

        <h3 class="text-lg font-semibold pt-2 my-2">
          Attention in Transformer Layers
        </h3>

        <p class="mb-2">
          Within each Transformer layer, the transformation of a token's
          vector representation isn't solely based on the token itself. The
          crucial "self-attention" mechanism allows each token to look at
          other tokens within the sequence (specifically, preceding tokens in
          a standard decoder setup like ours) and weigh their importance. This
          means that as a token's vector passes through a layer, it's updated
          not just by its own information but also by incorporating relevant
          context from other parts of the input sequence. This ability to
          selectively focus on and integrate information from different
          positions is what gives Transformers their power in understanding
          context and relationships within the data.
        </p>

        <p class="mb-2">
          Visualizing which tokens the attention mechanism focuses on when
          transforming each token reveals quite some interesting details about
          how the model processes the sequence.
        </p>

        <figure class="my-4">
          <!--TODO: poster-->
          <!--TODO: why 40 percent?-->
          <model-viewer class="w-full h-96" style="background-color: unset" interaction-prompt="none"
            field-of-view="30deg" disable-zoom="true" src="forward-complete.glb"
            environment-image="legacy" tone-mapping="none" exposure="0.5" touch-action="pan-y" disable-pan
            camera-controls camera-orbit="-67deg 72deg 40%" max-camera-orbit="auto 90deg auto" shadow-intensity="0.5"
            alt="some visualization"></model-viewer>
          <figcaption class="text-center text-sm mt-2">
            Visualization including attention connections (splines) between
            tokens within each Transformer layer. Different colors represent
            different attention heads. Only connections with weights above a
            threshold are shown.
          </figcaption>
          <p class="my-2">
            Observing the attention patterns (colored splines) reveals
            interesting details. In the first Transformer layer (middle row),
            the earliest significant attention (above a threshold) occurs when
            processing the third token, "<span class="font-mono text-xs">spicy</span>". It attends back to the preceding
            "<span class="font-mono text-xs">i</span>" token. This makes sense because "<span
              class="font-mono text-xs">spicy</span>" appears in multiple contexts within our small training dataset
            (e.g., "<span class="font-mono text-xs">chili tastes spicy</span>", "<span class="font-mono text-xs">spicy
              is a chili</span>",
            "<span class="font-mono text-xs">chili is so spicy</span>"). To
            correctly interpret or predict based on "<span class="font-mono text-xs">spicy</span>", the model benefits
            from looking at the preceding context. In
            contrast, the first token "<span class="font-mono text-xs">i</span>" shows no incoming attention lines
            because there are no prior
            tokens to attend to. The second token, "<span class="font-mono text-xs">like</span>", also shows no strong
            attention from "<span class="font-mono text-xs">i</span>". In our dataset, "<span
              class="font-mono text-xs">like</span>"
            consistently follows "<span class="font-mono text-xs">i</span>"
            but can precede various tastes ("<span class="font-mono text-xs">like spicy</span>", "<span
              class="font-mono text-xs">like sweet</span>", etc.).
            Therefore, knowing that "<span class="font-mono text-xs">i</span>"
            came before "<span class="font-mono text-xs">like</span>" provides
            little predictive value for what taste might follow, so the
            attention weight remains low.
          </p>
          <p class="my-2">
            The next token in the sequence is "<span class="font-mono text-xs">so</span>". In Transformer Layer 1
            (middle row), this token exhibits
            strong attention towards both the preceding token "<span class="font-mono text-xs">spicy</span>" and the
            initial token "<span class="font-mono text-xs">i</span>", indicated by the distinct colored splines
            connecting them
            (representing different attention heads). The focus on "<span class="font-mono text-xs">spicy</span>" is
            necessary because "<span class="font-mono text-xs">so</span>" appears before different subsequent tokens in
            the training data
            (e.g., "<span class="font-mono text-xs">so i like</span>", "<span class="font-mono text-xs">so sour</span>",
            "<span class="font-mono text-xs">so sweet</span>"), making the
            immediate preceding context crucial. The attention back to the
            initial "<span class="font-mono text-xs">i</span>" might help
            establish the overall sentence structure ("<span class="font-mono text-xs">i like ... so i like ...</span>")
            which is common in the training data.
          </p>
          <p class="my-2">
            Finally, let's examine the last token in the input sequence, the
            second "<span class="font-mono text-xs">like</span>". In both
            Transformer Layer 1 (middle row) and Transformer Layer 2 (top
            row), this token shows strong attention (indicated by splines)
            directed towards the token "<span class="font-mono text-xs">spicy</span>". This focus is crucial for the
            model's prediction. The training
            data contains similar sentences like "<span class="font-mono text-xs">i like sweet so i like apple</span>"
            or "<span class="font-mono text-xs">i like sour so i like lemon</span>". The key piece of information that
            distinguishes the current
            sequence and points towards "<span class="font-mono text-xs">chili</span>" as the correct completion is the
            word "<span class="font-mono text-xs">spicy</span>". The attention mechanism correctly identifies and
            utilizes this
            critical context from earlier in the sequence to inform the final
            prediction.
          </p>
        </figure>
      </section>

      <section class="mb-8">
        <h2 class="text-2xl font-semibold border-b pb-2 mb-4">Conclusion</h2>
        <p class="mb-2">
          By radically simplifying the dataset, tokenization, and model
          architecture, this article provided a step-by-step visualization of
          a decoder-only Transformer's internal workings. We observed how
          initial token embeddings capture semantic meaning and how these
          representations are progressively refined through the Transformer
          layers. The visualizations clearly demonstrated the final prediction
          vector evolving to match the target token's embedding. Furthermore,
          examining the attention mechanism revealed how the model selectively
          focuses on relevant prior tokens to inform its processing and
          predictions, successfully generalizing even from a minimal dataset.
          While highly simplified, this approach offers valuable intuition
          into the fundamental processes of information flow and contextual
          understanding within Transformer models.
        </p>
      </section>

      <section>
        <h2 class="text-2xl font-semibold border-b pb-2 mb-4">References</h2>
        <ul class="list-disc pl-5 text-sm">
          <li class="mb-1">
            Author, A. A., & Author, B. B. (Year). Title of work. Publisher.
          </li>
          <li class="mb-1">
            Author, C. C. (Year). Title of article. <em>Journal Title</em>,
            Volume(Issue), pages.
          </li>
          <li class="mb-1">
            Author, D. D. (Year). Title of chapter. In E. E. Editor & F. F.
            Editor (Eds.), <em>Title of book</em> (pp. pages). Publisher.
          </li>
        </ul>
      </section>
    </article>

    <footer class="mt-12 text-center text-sm text-gray-500">
      <p>&copy; 2025 Robert Timm/rtti.de All rights reserved.</p>
    </footer>
  </div>
</body>

</html>
